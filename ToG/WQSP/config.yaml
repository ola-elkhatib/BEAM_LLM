dataset: "WQSP" #choose dataset
kg_type: "half" #choose kg full or half
hops: 2 #choose the number of hops
max_length: 518 #the max length of LLMs output.
dataset_max: 10
boost_rel_score_from_graph: True
embedding_dimension: 256
temperature_exploration: 0.4 #the temperature in exploration stage.
temperature_reasoning: 0 #the temperature in reasoning stage.
entity_width: 5 #choose the search width of ToG.
rel_width: 3
depth: 2 #choose the search depth of ToG.
remove_unnecessary_rel: False #whether removing unnecessary relations.
relation_search_tool: 'rel_entity_dict' #llm Or of path_generation or rel_entity_dict
LLM_type: 'gpt-4'    #'gpt-3.5-turbo'
opeani_api_keys: 'xxxxxx'
num_retain_entity: 5 #Number of entities retained during entities search.
prune_tools: "llm" #prune tools for ToG, can be llm (same as LLM_type), bm25 or sentencebert.
traversal_space: embedding_space #graph_path or embedding_space
dropout: 0.2
do_batchnorm: True
do_dropout: True
device: 'cpu'
wandb_use: False
constraints_refuse: False
debug: True
wandb_name: "BeamQA+ToG"
wandb_project: "BeamQA+ToG"


